# ðŸŽ“ AI Teaching Assistant for Faculty

### Agentic RAG-based Auto Planning & Auto Assessment System

---

## ðŸ“Œ Problem Context (Why This Solution?)

In real academic environments, faculty members are responsible for multiple **manual, repetitive, and regulation-heavy tasks**, such as:

* Analyzing large syllabus PDFs
* Planning weekly lesson schedules
* Designing question papers aligned with Bloomâ€™s Taxonomy
* Evaluating student answers using rubrics

Existing AI tools behave like **generic chatbots**:

* âŒ They hallucinate content
* âŒ They ignore syllabus boundaries
* âŒ They do not follow Bloomâ€™s taxonomy or exam rules
* âŒ They provide no academic compliance guarantees

ðŸ‘‰ **Hence, a chatbot-style AI is unsafe for academic workflows.**

---

## ðŸ’¡ Proposed Solution (What We Built)

We built an **Agentic AI Teaching Assistant** that behaves like a **digital academic assistant**, not a chatbot.

The system:

* Works strictly within **syllabus boundaries**
* Enforces **Bloomâ€™s Taxonomy**
* Follows **exam patterns & rubrics**
* Uses **multi-agent reasoning + guardrails** to ensure correctness

This makes the solution **safe, explainable, and suitable for universities**.

---

## ðŸ§  How the Solution Works (End-to-End Flow)

The solution follows a **controlled, step-by-step academic pipeline** instead of a single AI prompt.

```
Faculty Input
   â†“
Intent Understanding Agent
   â†“
Orchestrator (Decision Maker)
   â†“
RAG (Syllabus Retrieval)
   â†“
Academic Content Agents
   â†“
Compliance & Validation Agent
   â†“
Final Output to Faculty
```

Each step is **independently verified**, making the system reliable.

---

## ðŸ—ï¸ System Architecture Overview

The architecture is designed around **agent separation of concerns**.

### Core Principles

* One agent = one academic responsibility
* Agents do not directly talk to each other
* Orchestrator controls execution flow
* All outputs are validated before display

![Architecture Diagram](https://github.com/Jasmikapeddada/TEAM-60/blob/main/ChatGPT%20Image%20Dec%2013%2C%202025%2C%2004_40_33%20PM.png)

---

## ðŸ§  Updated Agent Design (Modified from Initial Version)

To improve **clarity, academic safety, and jury explainability**, we refined the originally used agents into a **cleaner, responsibility-driven design**. Each agent now has:

* A **single academic responsibility**
* **Structured JSON input/output**
* No direct inter-agent coupling

---

### 1ï¸âƒ£ Intent Understanding Agent (NEW â€“ Added)

**Why modified?**
Earlier, user input was passed directly to generation agents, which caused ambiguity.

**Updated Role:**

* Converts faculty natural language into structured intent
* Decides *what* needs to be generated (lesson plan, questions, evaluation)

**Example Output:**

```json
{
  "tasks": ["lesson_plan", "question_paper"],
  "units": ["Unit-2"],
  "weeks": 6,
  "bloom_levels": ["Understand", "Apply"]
}
```

ðŸ‘‰ This agent enables **dynamic orchestration**.

---

### 2ï¸âƒ£ Orchestrator Agent (ENHANCED)

**Earlier:** Static execution flow

**Now:**

* Dynamically decides which agents to invoke
* Controls async execution
* Maintains execution timeline

**Responsibility:**

* No content generation
* Only routing, coordination, and logging

---

### 3ï¸âƒ£ Syllabus Understanding Agent (REFINED)

**Earlier:** Raw text extraction

**Now:**

* Converts syllabus PDF into structured JSON
* Extracts:

  * Units
  * Topics
  * Learning outcomes

**Why modified?**
This structured syllabus becomes the **single source of truth** for all agents.

---

### 4ï¸âƒ£ Lesson Planner Agent (SPLIT FROM CONTENT AGENT)

**Earlier:** Lesson planning + generation combined

**Now (Separated):**

* Allocates topics week-wise
* Ensures syllabus coverage balance
* Prevents topic overload

ðŸ‘‰ Separation improves explainability and debugging.

---

### 5ï¸âƒ£ Assessment Generator Agent (REFINED)

**Earlier:** Generic question generation

**Now:**

* Generates questions per Bloom level
* Enforces exam pattern rules
* Uses Bloom verbs from constants

**Guarantees:**

* No repetition
* Balanced difficulty
* Bloom compliance

---

### 6ï¸âƒ£ Evaluation Agent (ENHANCED)

**Earlier:** Simple scoring

**Now:**

* Rubric-based scoring
* Criterion-wise feedback
* Missing concept detection

**Output Includes:**

* Marks
* Strengths
* Weak areas
* Suggestions

---

### 7ï¸âƒ£ Compliance Agent (NEW â€“ Non-Generative)

**Why added?**
To ensure academic safety before final output.

**Checks:**

* Out-of-syllabus content
* Bloom imbalance
* Exam rule violations

**Behavior:**

* PASS â†’ Output displayed
* FAIL â†’ Regeneration triggered

ðŸ‘‰ This agent provides **institution-level trust**.

---

## ðŸ“š RAG Pipeline (Why Answers Are Trustworthy)

### Step 1: Document Ingestion

* Syllabus PDF â†’ text extraction
* Chunked into 500â€“700 tokens
* Embedded using SentenceTransformers

### Step 2: Vector Storage

* Stored in FAISS Vector DB

### Step 3: Context Retrieval

* Retrieves only relevant syllabus chunks
* Filters by unit and topic

### Step 4: Grounded Generation

* LLM generates output using retrieved context only

ðŸ‘‰ **No retrieved context = no generation**

---

## ðŸ›¡ï¸ Academic Guardrails

### Hard Guardrails (Non-negotiable)

* Syllabus-only generation
* Bloom taxonomy enforcement
* Exam pattern adherence

### Soft Guardrails (Quality Enhancements)

* Difficulty balancing
* Concept diversity
* Verb variation

---

## ðŸ“Š Evaluation Metrics (How Quality Is Measured)

| Metric                | Description                                       |
| --------------------- | ------------------------------------------------- |
| Bloom Alignment Score | Match between expected and generated Bloom levels |
| Coverage Completeness | % of syllabus topics covered                      |
| Difficulty Balance    | Variance across question difficulty               |
| Explainability Score  | Quality of evaluation feedback                    |

---

## ðŸ–¥ï¸ Streamlit UI Walkthrough

### 1ï¸âƒ£ Upload Page

* Upload syllabus PDF
* Select subject & output type

### 2ï¸âƒ£ Processing Page

* Live agent execution status
* Intermediate outputs shown

### 3ï¸âƒ£ Results Page

* Lesson plan table
* Question paper
* Bloom taxonomy distribution
* Answer evaluation feedback

---

## ðŸ› ï¸ Technology Stack

| Layer      | Technology           |
| ---------- | -------------------- |
| UI         | Streamlit            |
| Agents     | LangChain / CrewAI   |
| LLM        | Groq / OpenAI        |
| Embeddings | SentenceTransformers |
| Vector DB  | FAISS                |
| Backend    | Python               |

---

## ðŸŽ¯ Key Differentiators

âœ” Not a chatbot
âœ” Fully syllabus-grounded (RAG)
âœ” Multi-agent academic reasoning
âœ” Deterministic compliance checks
âœ” University-ready design

---

## ðŸŽ¤ Jury-Friendly One-Line Pitch

> "An agentic, syllabus-grounded AI Teaching Assistant that automates academic planning and assessment while enforcing Bloomâ€™s taxonomy, exam rules, and compliance â€” something a normal chatbot cannot do."

---

## ðŸ”® Future Scope

* LMS integration
* Multilingual syllabus support
* Department-level analytics
* Adaptive learning recommendations
